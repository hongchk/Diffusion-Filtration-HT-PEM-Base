import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence
from torch.nn import functional as F
from collections import Counter
from torch import nn, optim
from torch.optim.lr_scheduler import StepLR
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn import linear_model
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from rdkit import Chem
from rdkit.ML.Descriptors import MoleculeDescriptors
from rdkit.Chem import Descriptors
from rdkit.Chem import PDBWriter
from sklearn.metrics import mean_squared_error as mse
import shap
import random
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import joblib
import sys
import os
from matplotlib.cm import RdBu
import seaborn as sns


class StringDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def q_x(x_0, t, betas):
    alphas = 1 - betas
    alphas_prod = torch.cumprod(alphas, 0)
    alphas_prod_p = torch.cat([torch.tensor([1]).float(), alphas_prod[:-1]], 0) 
    alphas_bar_sqrt = torch.sqrt(alphas_prod)
    one_minus_alphas_bar_log = torch.log(1 - alphas_prod)
    one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)
    noise = torch.randn_like(x_0)
    alphas_t = alphas_bar_sqrt[t]
    alphas_1_m_t = one_minus_alphas_bar_sqrt[t]
    return (alphas_t * x_0 + alphas_1_m_t * noise)

class MLPDiffusion(nn.Module):

    def __init__(self, n_steps, max_len, num_units=128):
        super(MLPDiffusion, self).__init__()
        self.linears = nn.ModuleList(
            [
                nn.Linear(max_len, num_units), 
                nn.ReLU(), 
                nn.Linear(num_units, num_units),
                nn.ReLU(), 
                nn.Linear(num_units, num_units),
                nn.ReLU(), 
                nn.Linear(num_units, max_len),
            ]
        )
        self.step_embeddings = nn.ModuleList(
            [
                nn.Embedding(n_steps, num_units), 
                nn.Embedding(n_steps, num_units),
                nn.Embedding(n_steps, num_units),
            ]
        )

    def forward(self, x_0, t):
        x = x_0
        for idx, embedding_layer in enumerate(self.step_embeddings):
            t_embedding = embedding_layer(t)
            x = self.linears[2*idx](x)
            x += t_embedding
            x = self.linears[2*idx+1](x)
            
        x = self.linears[-1](x)

        return x 

def diffusion_loss_fn(model, x_0, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, n_steps):
    batch_size = x_0.shape[0]
    t = torch.randint(0, n_steps, size=(batch_size,))
    t = torch.cat([t, n_steps-1-t], dim=0) #[batchsize]
    t = t.unsqueeze(-1) #[batchsize, 1]
    a = alphas_bar_sqrt[t]
    am1 = one_minus_alphas_bar_sqrt[t]
    e = torch.randn_like(x_0)
    x = x_0*a + e*am1
    output = model(x, t.squeeze(-1))
    return (e - output).square().mean()

def text_to_sequence(text):
    return [vocab[char] for char in text if char != ', ']

def p_sample_loop(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt):
    cur_x = torch.randn(shape)
    x_seq = [cur_x]
    for i in reversed(range(n_steps)):
        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt)
        x_seq.append(cur_x)
    return x_seq

def p_sample(model, x, t, betas, one_minus_alphas_bar_sqrt):
    t = torch.tensor([t])
    coeff = betas[t] / one_minus_alphas_bar_sqrt[t]
    eps_theta = model(x, t)
    mean = (1 / (1-betas[t]).sqrt()) * (x - (coeff * eps_theta))
    z = torch.randn_like(x)
    sigma_t = betas[t].sqrt()
    sample = mean + sigma_t *z 
    return (sample)

def stringsplit(instring):
    return [part for part in instring.split('[*]') if part]

def ring_num_change(instring, ringnum):
    rn=[]
    rn_dic={}
    for s in instring:
        try:
            num = int(s)
            if num not in rn:
                rn.append(num)
                rn_dic[str(num)] = str(ringnum)
                ringnum += 1
        except:
            pass
    outstring = ''.join(rn_dic.get(c, c) for c in instring)
    return outstring, ringnum

def add1(generatelist, addstring, count, addcode, ringnum):
    newaddstring, ringnum = ring_num_change(addstring, ringnum)
    addlist = stringsplit(newaddstring)
    if addlist[-1][0] == '.':
        generatelist.insert(len(generatelist)+10, addlist[-1])
        addlist = addlist[:-1]
    for i in addlist:
        generatelist.insert(count, i)
        count += 1
    count += (2-int(addcode[0]))
    #print(addcode[0])
    #print(count)
    return generatelist, count, ringnum

def addall(allbead, bead_DF):
    generatelist = []
    ringnum = 1
    count = 0
    for bead in allbead:
        addstring = bead_DF.loc[bead_DF['code']==bead, 'SMILES'].values[0]
        generatelist, count, ringnum = add1(generatelist, addstring, count, bead, ringnum)
    return generatelist

def RDkit_descriptors(smiles):
    mols = [Chem.MolFromSmiles(i) for i in smiles]
    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])
    desc_names = calc.GetDescriptorNames()
    n=0
    Mol_descriptors = []
    for mol in mols:
        # add hydrogens to molecules
        n+=1
        try:
            mol = Chem.AddHs(mol)
        except:
            print(n)
            exit()
        # Calculate all 200 descriptors for each molecule
        descriptors = calc.CalcDescriptors(mol)
        Mol_descriptors.append(descriptors)
    return Mol_descriptors, desc_names

def generate_poly(model, generated_num, max_len, num_step, betas, one_minus_alphas_bar_sqrt):
    x_seq = p_sample_loop(model, [generated_num, max_len], num_steps, betas, one_minus_alphas_bar_sqrt)
    return x_seq[-1]

def traslate_generate(result, vocab, inv_vocab):
    created=[]
    for n in result:
        created_string=[]
        for i in n:
            count = 0
            #print(i.item())
            if round(i.item()) > len(vocab):
            #created_string.append(max(round(i.item()),0))
                count = 1
            #print('-1')
                break
            elif 32 >= round(i.item()) > 0:
                created_string.append(inv_vocab[round(i.item())])
        if count <= 0:
            created.append(created_string)
    return created

def linear_polymer(created):
    linear_polymer = []
    for i in created:
        poly=2
        num_chem = 0
        for j in i:
            count=int(j[0])
            poly += (count-2)
            num_chem += 1
            if poly < 2:
                poly -= 999
        if num_chem <= 0:
            poly -= 9999
        if poly == 2:
            linear_polymer.append(i)
    return linear_polymer

def generate_SMILES(linear_polymer, BEAD_SMILES):
    generate_SMILES = []
    #print(linear_polymer)
    for poly in linear_polymer:
        #print(poly)
        single_SMILES = ''
        gene = addall(poly, BEAD_SMILES)
        for i in gene:
            single_SMILES += i 
        generate_SMILES.append(single_SMILES)
        #print(single_SMILES)
    return generate_SMILES

def generate_polymer(model=model, generated_num=1000, max_len=max_len, num_step=num_steps, 
                    betas=betas, one_minus_alphas_bar_sqrt=one_minus_alphas_bar_sqrt, 
                   vocab=vocab, inv_vocab=inv_vocab, BEAD_SMILES=BEAD_SMILES):
    generate_result = generate_poly(model, generated_num, max_len, num_step, betas, one_minus_alphas_bar_sqrt)
    created = traslate_generate(generate_result, vocab, inv_vocab)
    linear_poly = linear_polymer(created)
    new_linear_poly = []
    for i in linear_poly:
        includepoly = 1
        for group in range(len(i)-1):
            if i[group] == '2001' and i[group+1] == '2001':
                includepoly = 0
        #if '2301' not in i and includepoly == 1:
        if includepoly == 1:
                    new_linear_poly.append(i)
    new_SMILES = generate_SMILES(new_linear_poly, BEAD_SMILES)
    return new_SMILES,new_linear_poly

def needed_output(thick, PAU, T, proton, joblib_model, target=1, columns=['D','PA','T','VSAE3','VSAE2','EVSA8','MESI','BJ','para_H'], 
                 select_B_col = ['VSA_EState3', 'VSA_EState2', 'EState_VSA8', 'MinEStateIndex', 'BalabanJ', 'fr_para_hydroxylation'] ):
    
    needed_SMILES=[]
    needed_proton = []
    needed_BEADS = []
    count = 0
    cycle = 0

    saved_model = joblib.load(joblib_model)
    feature_scaler = saved_model["feature_scaler"]
    target_scaler = saved_model["target_scaler"]
    RF = saved_model["model"]
    while count < target:
        SMILES,BEADS = generate_polymer()
        Mol_descriptors_create, desc_names = RDkit_descriptors(SMILES)
        
        general_list = [[thick, PAU, T]]*len(Mol_descriptors_create)
        general_columns = ['thickness','PA uptake','temperature']
        general_DF = pd.DataFrame(general_list, columns=general_columns)
    
        generate_B_SMILES_DF = pd.DataFrame(Mol_descriptors_create, columns=desc_names)
        generate_B_SMILES_select = generate_B_SMILES_DF[select_B_col]
        generate_selected = pd.concat([general_DF, generate_B_SMILES_select], axis=1)

        generate_X = feature_scaler.transform(generate_selected)
        generate_X = pd.DataFrame(generate_X, columns=columns)
        #predict
        generate_proton_scaled = RF.predict(generate_X)
        generate_proton = target_scaler.inverse_transform(generate_proton_scaled.reshape(-1,1))
        cycle +=1
        print(cycle)

        for i in range(len(SMILES)):
            if generate_proton[i][0] >= proton:
                count += 1
                needed_SMILES.append(SMILES[i])
                needed_BEADS.append(BEADS[i])
                needed_proton.append(generate_proton[i][0])
        
    return needed_SMILES, needed_proton, needed_BEADS
        


model = torch.load('the model')
data = pd.read_excel('the data')
texts = data['NEW_SMILES'].astype(str).tolist()

all_chars_list = []
for text in texts:
    chars = text.split(', ')
    all_chars_list.append(chars)

all_chars = [char for list in all_chars_list for char in list]
char_counter = Counter(all_chars)
vocab = {char: idx + 1 for idx, (char, _) in enumerate(char_counter.items())}  # +1 to reserve 0 for padding
vocab_size = len(vocab) + 1  # Add 1 for padding token

inv_vocab = {idx: char for char, idx in vocab.items()}

generated_num = 1000
max_len = 30
num_steps = 2000
betas = torch.linspace(-6, 6, num_steps)
betas = torch.sigmoid(betas) * (0.5e-2 - 1e-5) + 1e-5

alphas = 1 - betas
alphas_prod = torch.cumprod(alphas, 0)
alphas_prod_p = torch.cat([torch.tensor([1]).float(), alphas_prod[:-1]], 0) #p表示为previous
alphas_bar_sqrt = torch.sqrt(alphas_prod)
one_minus_alphas_bar_log = torch.log(1 - alphas_prod)
one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)

BEAD_SMILES = pd.read_excel('beed-code-SMILES').astype(str)

SMILES,BEADS = generate_polymer()
beadlist=[]
for a in BEADS:
    for b in a:
        beadlist.append(b)

needed_SMILES,needed_proton,needed_BEADS = needed_output(thick, PAU, T, proton, joblib_model, target)




